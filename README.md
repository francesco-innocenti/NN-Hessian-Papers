# Neural Network Hessian Papers
A curated repository of papers on the Hessian of neural networks.

The Hessian of neural networks, or more specifically of the loss with respect to
model parameters, is a central object of study in deep learning for both 
optimisation and generalisation. During my PhD, I happened to read a lot about
the Hessian so here's my literature review.


## Early empirical studies
[Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond
](https://arxiv.org/abs/1611.07476): 
One of the first empirical studies of the Hessian observing its now 
well-recognised singularity with a spectrum centered around zero and a few outliers
depending on the data.

[Identifying and attacking the saddle point problem in high-dimensional non-convex optimization
](https://proceedings.neurips.cc/paper/2014/hash/17e23e50bedc63b4095e3d8204ce063b-Abstract.html)

[Empirical Analysis of the Hessian of Over-Parametrized Neural Networks
](https://arxiv.org/abs/1706.04454)

[The Full Spectrum of Deepnet Hessians at Scale: Dynamics with SGD Training and Sample Size
](https://arxiv.org/abs/1811.07062)

[Negative eigenvalues of the Hessian in deep neural networks
](https://arxiv.org/abs/1902.02366)

[Gradient Descent Happens in a Tiny Subspace
](https://arxiv.org/abs/1812.04754)

[Does SGD really happen in tiny subspaces?
](https://arxiv.org/abs/2405.16002)

[An Investigation into Neural Net Optimization via Hessian Eigenvalue Density
](https://proceedings.mlr.press/v97/ghorbani19b)

[How noise affects the Hessian spectrum in overparameterized neural networks
](https://arxiv.org/abs/1910.00195)

## Flatness & generalisation
[On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima
](https://arxiv.org/abs/1609.04836):


